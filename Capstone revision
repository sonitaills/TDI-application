#OBS: Se colocarmos o colision_ID como índice, ele não vai ser único! Quando temm mais de uma pessoa ferida
#o mesmo colision ID vem do dataset de pessoas (faz sentido)
#-------------------------------------------------------------------------------------------------------------
import pandas as pd
import numpy as np

crashes = pd.read_csv('Motor_Vehicle_Collisions_-_Crashes.csv', index_col = 'COLLISION_ID' )

#Adding year / month columns to dataset
crashes['CRASH DATE'] = pd.to_datetime(crashes['CRASH DATE'])
crashes['year'] = crashes['CRASH DATE'].dt.year
crashes['month'] = crashes['CRASH DATE'].dt.month

#How can I map the crash place?
#Data with mapping opportunity
places = ['BOROUGH','ON STREET NAME','ZIP CODE','LATITUDE','LONGITUDE','year','month','NUMBER OF PERSONS INJURED',
       'NUMBER OF PERSONS KILLED', 'NUMBER OF PEDESTRIANS INJURED',
       'NUMBER OF PEDESTRIANS KILLED', 'NUMBER OF CYCLIST INJURED',
       'NUMBER OF CYCLIST KILLED', 'NUMBER OF MOTORIST INJURED',
       'NUMBER OF MOTORIST KILLED']
known_places = crashes[places].dropna(subset = places[1:],how ='all')
crashes_size = crashes.shape[0]
#Ammount of know location
known_streets = crashes['ON STREET NAME'].dropna().count()
known_zip_codes = crashes['ZIP CODE'].dropna().count()
known_coordinates = crashes['LATITUDE'].dropna().count()
print('Ammount of data with known streets = {}, corresponding to {:.2f}% of dataset '.format(known_streets,known_streets/known_places.shape[0]*100))
print('Ammount of data with known codes = {}, corresponding to {:.2f}% of dataset '.format(known_zip_codes,known_zip_codes/known_places.shape[0]*100))
print('Ammount of data with known coordinates = {}, corresponding to {:.2f}% of dataset '.format(known_coordinates,known_coordinates/known_places.shape[0]*100))
# Here should be a graph showing the most common mapping type used and focus of this analysis

#How much data do we have mapped via coordinates but no borough data?
unk_bor = known_places[known_places['BOROUGH'].isna()]
unk_bor_kno_coor = known_places[known_places['BOROUGH'].isna() & known_places['LATITUDE'].notna()]
print('Ammount of data with unknown borough = {}, corresponding to {:.2f}% of dataset '.format(len(unk_bor),len(unk_bor)/known_places.shape[0]*100))
print('Ammount of data with unknown borough but with known coordinates = {}, corresponding to {:.2f}% of dataset '.format(len(unk_bor_kno_coor),len(unk_bor_kno_coor)/known_places.shape[0]*100))


# On the cell above I have mapped the ammount of data that has means of identifying the BOROUGH, but it is not currently there.
#Now I'll use only the ones that are already mapped
acc_with_borough = crashes[places].dropna(subset = ['BOROUGH'])
know_borough_percent = acc_with_borough.shape[0]/crashes_size
print('Known borough = {}, corresponding to {:.2f} % of accidents'.format(acc_with_borough.shape[0],know_borough_percent))
know_borough_indices = list(acc_with_borough.index) #This indices will be passed to the other dataframes

by_bor_by_year = acc_with_borough.groupby(['BOROUGH','year']).apply(lambda x: len(x))
unstack_test= by_bor_by_year.unstack()
#Esta tabela mostra a distribuição de acidentes por bairro / por ano
print('# of accidents')
display(unstack_test)
inj_kil_dict = {'INJURED':'NUMBER OF PERSONS INJURED','KILLED':'NUMBER OF PERSONS KILLED'}
injury_accidents = pd.pivot_table(acc_with_borough, values= inj_kil_dict['INJURED'], index=['BOROUGH','year'],aggfunc= np.nansum).unstack()
print('# of injured people')
display(injury_accidents)
print('# of killed people')
fatal_accidents = pd.pivot_table(acc_with_borough, values= inj_kil_dict['KILLED'], index=['BOROUGH','year'],aggfunc= np.nansum).unstack()
display(fatal_accidents)

import matplotlib.pyplot as plt
# Pie chart, where the slices will be ordered and plotted counter-clockwise:
injury_accidents['sum'] = injury_accidents[injury_accidents.columns].apply(lambda x: round(np.sum(x)),axis =1)
labels = list(injury_accidents.index)

explode = (0, 0.2, 0.2, 0.2, 0)  # only "explode" the 2nd slice (i.e. 'Hogs')
sizes = injury_accidents['sum']

fig, axs = plt.subplots(1,3,figsize=(20,5))
fig.suptitle("Severity distribution", fontsize=18)
plt.subplots_adjust(top=0.8)

axs[2].pie(sizes, explode=explode, autopct='%1.1f%%',
        shadow=True, startangle=90)
axs[2].axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
axs[2].set_title('injuries')

sizes = fatal_accidents[fatal_accidents.columns].apply(lambda x: round(np.sum(x)),axis =1)
axs[1].pie(sizes, explode=explode, autopct='%1.1f%%',
        shadow=True, startangle=90, labeldistance=0.9)
axs[1].axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
axs[1].set_title('fatalities')

sizes = unstack_test[unstack_test.columns].apply(lambda x: round(np.sum(x)),axis =1)
axs[0].pie(sizes, explode=explode, autopct='%1.1f%%',
        shadow=True, startangle=90, labeldistance=0.9)
axs[0].axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
axs[0].set_title('Total accidents')


plt.legend(loc="best",bbox_to_anchor=(1.8,0.8), title="Borough", fancybox=True, ncol=1, shadow=True, 
           fontsize = 'large', title_fontsize ='x-large',labels=labels)
plt.show()

import scipy.stats

motorist_accidents = pd.pivot_table(acc_with_borough, values= 'NUMBER OF MOTORIST INJURED', index=['BOROUGH','year'],aggfunc= np.nansum).unstack()
motorist_accidents['mean (w/o 2021)'] = motorist_accidents[motorist_accidents.columns[:-1]].apply(lambda x: round(np.mean(x)),axis =1)
motorist_accidents['stdev (w/o 2021)'] = motorist_accidents[motorist_accidents.columns[:-2]].apply(lambda x: round(np.std(x)),axis =1)
motorist_accidents['coef_var % (w/o 2021)'] = motorist_accidents[motorist_accidents.columns[:-3]].apply(lambda x: round(scipy.stats.variation(x)*100),axis =1)
display(motorist_accidents)

pedestrian_accidents = pd.pivot_table(acc_with_borough, values= 'NUMBER OF PEDESTRIANS INJURED', index=['BOROUGH','year'],aggfunc= np.nansum).unstack()
pedestrian_accidents['mean (w/o 2021)'] = pedestrian_accidents[pedestrian_accidents.columns[:-1]].apply(lambda x: round(np.mean(x)),axis =1)
pedestrian_accidents['stdev (w/o 2021)'] = pedestrian_accidents[pedestrian_accidents.columns[:-2]].apply(lambda x: round(np.std(x)),axis =1)
pedestrian_accidents['coef_var % (w/o 2021)'] = pedestrian_accidents[pedestrian_accidents.columns[:-3]].apply(lambda x: round(scipy.stats.variation(x)*100),axis =1)
display(pedestrian_accidents)

cyclist_accidents = pd.pivot_table(acc_with_borough, values= 'NUMBER OF CYCLIST INJURED', index=['BOROUGH','year'],aggfunc= np.nansum).unstack()
cyclist_accidents['mean (w/o 2021)'] = cyclist_accidents[cyclist_accidents.columns[:-1]].apply(lambda x: round(np.mean(x)),axis =1)
cyclist_accidents['stdev (w/o 2021)'] = cyclist_accidents[cyclist_accidents.columns[:-2]].apply(lambda x: round(np.std(x)),axis =1)
cyclist_accidents['coef_var % (w/o 2021)'] = cyclist_accidents[cyclist_accidents.columns[:-3]].apply(lambda x: round(scipy.stats.variation(x)*100),axis =1)
display(cyclist_accidents)

def bar_plot(data_frame,kind_accident):

    fig, axs = plt.subplots(1,3,figsize=(30,8))
    fig.suptitle(kind_accident, fontsize=18)
    plt.subplots_adjust(top=0.9)

    xpos = np.arange(len(years))
    plt.yticks(fontsize=15)
    for i,df in enumerate(data_frame):
        if i==0:
            axs[i].set_xlabel('years',fontsize=30)
            axs[i].set_ylabel('# victms',fontsize=30)
        axs[i].set_title(person_type[i])
        axs[i].tick_params(axis='both', labelsize=15)
        axs[i].set_xticks(xpos)
        axs[i].set_xticklabels(years,rotation=45)
        axs[i].grid(color='k', linestyle='-.',axis = 'y',linewidth=0.5, zorder=0)


        for loop, region in enumerate (color_dict):
            accidents = df.xs(region)[:-4]
            if boroughs[loop] != 'STATEN ISLAND' and boroughs[loop] != 'BRONX':
                axs[i].bar(xpos+0.15*loop,accidents,width=0.2, color= color_dict[region],label=region,zorder=6)
        
    plt.legend(loc="best",bbox_to_anchor=(-2.6,0.8), fancybox=True, ncol=1, shadow=True, 
               fontsize = 'x-large')
               
               years = motorist_accidents.columns.get_level_values(1)[:-4] #pegando entre 2012 e 2020
#years.append['mean (w/o 2021)']
boroughs = list(acc_with_borough['BOROUGH'].unique())

color_dict = {boroughs[0]:'red',
             boroughs[1]:'blue',
             boroughs[2]:'green',
             boroughs[3]:'orange',
             boroughs[4]:'purple'}

person_dataframe =[motorist_accidents,pedestrian_accidents,cyclist_accidents]
person_type= ['VEHICLE OCCUPANT','PEDESTRIAN','CYCLIST']

bar_plot(person_dataframe,"INJURIES")

motorist_fatalities = pd.pivot_table(acc_with_borough, values= 'NUMBER OF MOTORIST KILLED', index=['BOROUGH','year'],aggfunc= np.nansum).unstack()
motorist_fatalities['mean (w/o 2021)'] = motorist_fatalities[motorist_fatalities.columns[:-1]].apply(lambda x: round(np.mean(x)),axis =1)
motorist_fatalities['stdev (w/o 2021)'] = motorist_fatalities[motorist_fatalities.columns[:-2]].apply(lambda x: round(np.std(x)),axis =1)
motorist_fatalities['coef_var % (w/o 2021)'] = motorist_fatalities[motorist_fatalities.columns[:-3]].apply(lambda x: round(scipy.stats.variation(x)*100),axis =1)
display(motorist_fatalities)

pedestrian_fatalities = pd.pivot_table(acc_with_borough, values= 'NUMBER OF PEDESTRIANS KILLED', index=['BOROUGH','year'],aggfunc= np.nansum).unstack()
pedestrian_fatalities['mean (w/o 2021)'] = pedestrian_fatalities[pedestrian_fatalities.columns[:-1]].apply(lambda x: round(np.mean(x)),axis =1)
pedestrian_fatalities['stdev (w/o 2021)'] = pedestrian_fatalities[pedestrian_fatalities.columns[:-2]].apply(lambda x: round(np.std(x)),axis =1)
pedestrian_fatalities['coef_var % (w/o 2021)'] = pedestrian_fatalities[pedestrian_fatalities.columns[:-3]].apply(lambda x: round(scipy.stats.variation(x)*100),axis =1)
display(pedestrian_fatalities)

cyclist_fatalities = pd.pivot_table(acc_with_borough, values= 'NUMBER OF CYCLIST KILLED', index=['BOROUGH','year'],aggfunc= np.nansum).unstack()
cyclist_fatalities['mean (w/o 2021)'] = cyclist_fatalities[cyclist_fatalities.columns[:-1]].apply(lambda x: round(np.mean(x)),axis =1)
cyclist_fatalities['stdev (w/o 2021)'] = cyclist_fatalities[cyclist_fatalities.columns[:-2]].apply(lambda x: round(np.std(x)),axis =1)
cyclist_fatalities['coef_var % (w/o 2021)'] = cyclist_fatalities[cyclist_fatalities.columns[:-3]].apply(lambda x: round(scipy.stats.variation(x)*100),axis =1)
display(cyclist_fatalities)

person_dataframe =[motorist_fatalities,pedestrian_fatalities,cyclist_fatalities]

bar_plot(person_dataframe,"FATALITIES")

#Zip database found in https://www.health.ny.gov/statistics/cancer/registry/appendix/neighborhoods.htm
zip_database= pd.read_excel("NYC_zip_codes.xlsx")
zip_database['BOROUGH'] = zip_database['BOROUGH'].str.upper()
zip_database['zip code'] = zip_database['zip code'].apply(lambda x: str(x))
#Zip code 11249 was not present in database. Had to be added manually to Williamsburg
zip_database.iloc[17]['zip code'] = zip_database.iloc[17]['zip code']+',11249' 

motorist_zip_codes = pd.pivot_table(acc_with_borough, values= 'NUMBER OF MOTORIST INJURED', index=['BOROUGH','ZIP CODE'],aggfunc= np.nansum).unstack()
passenger_zip_codes = pd.pivot_table(acc_with_borough, values= 'NUMBER OF PEDESTRIANS INJURED', index=['BOROUGH','ZIP CODE'],aggfunc= np.nansum).unstack()
cyclist_zip_codes = pd.pivot_table(acc_with_borough, values= 'NUMBER OF CYCLIST INJURED', index=['BOROUGH','ZIP CODE'],aggfunc= np.nansum).unstack()

#These dictionaries stores the zip codes which represents 80% of the injuries
top_80_percent_drivers = {}
top_80_percent_pedestrians = {}
top_80_percent_cyclists = {}

#These dictionaries stores 80% of the injuries by sub region
drivers_sub_regions = {}
pedestrian_sub_regions = {}
cyclist_sub_regions = {}

#connecting all of the dictionaries to run in a nested for loop
top_connection = {'VEHICLE OCCUPANT':[top_80_percent_drivers,drivers_sub_regions,motorist_zip_codes],
                  'PEDESTRIAN':[top_80_percent_pedestrians,pedestrian_sub_regions,passenger_zip_codes],
                  'CYCLIST':[top_80_percent_cyclists,cyclist_sub_regions,cyclist_zip_codes]}

    
def return_sub_region(x): # connects zip codes to sub regions (zip_database)
    x = str(int(x))
    row_number =  zip_database.where(zip_database['zip code'].str.contains(x)).dropna().index[0]
    return zip_database.iloc[row_number]['region']

for subject in person_type:
    for loop, region in enumerate (boroughs):
        current_bourough = top_connection[subject][2].xs(region).dropna()
        current_bourough = current_bourough[current_bourough.values>0]

        #Identificando quais os zips concentram 80% dos machucados
        top_25_perc = current_bourough.to_frame().sort_values(by=[region],ascending = False)
        top_25_perc['cum_percent'] = 100*(top_25_perc[region].cumsum() / top_25_perc[region].sum()) 
        top_25_perc = top_25_perc[top_25_perc['cum_percent']<=80]
        top_25_perc.reset_index(level='ZIP CODE',inplace = True)

        top_connection[subject][0][region] = top_25_perc
        top_connection[subject][0][region].columns= top_connection[subject][0][region].columns.str.strip()
        top_connection[subject][0][region]['sub_region'] = top_connection[subject][0][region]['ZIP CODE'].apply(lambda x: return_sub_region(x))

        #subset for plotting
        top_connection[subject][1][region] = top_connection[subject][0][region].groupby("sub_region").agg({region:np.sum})
        top_connection[subject][1][region]  = top_connection[subject][1][region].sort_values(by=[region],ascending = False)
        top_connection[subject][1][region]['cum_percent'] = 100*(top_connection[subject][1][region].cumsum() / top_connection[subject][1] [region].sum())
        
        

for subject in person_type: 
    fig, axs = plt.subplots(1,5,figsize=(20,8))
    fig.suptitle(subject, fontsize=18, y=1.05)
    plt.subplots_adjust(top=0.3)
    for item,region in enumerate(boroughs):
        sub_regions = [x for x in top_connection[subject][1][region].index]
        cum_sum = [x for x in top_connection[subject][1][region]['cum_percent']]
        accidents = top_connection[subject][1][region][region]
        ypos = np.arange(len(sub_regions))
        axs[item].set_title(region)
        axs[item].set_xticks(ypos)
        axs[item].set_xticklabels(sub_regions,rotation=90,fontsize=15)
        axs[item].bar(ypos,accidents,width=0.8, color= color_dict[region])
        ax2 = axs[item].twinx()
        ax2.plot(ypos, cum_sum, 'k--', linewidth = 2, marker='o')

        if item==0:
            axs[item].set_ylabel('# injuries',fontsize=20)
        if item==4:
            ax2.set_ylabel('Cumulative percentage [%]',fontsize=20)

        fig.tight_layout()
        
        #Need now to import the people dataset
people = pd.read_csv('Motor_Vehicle_Collisions_-_Person.csv')
people = people[people.PERSON_TYPE.notna()] #removing row w/o info about victm (just 1 row)
people.set_index('COLLISION_ID', inplace = True)

#Joining people dataset with known borough_locations
wanted_from_people = ['UNIQUE_ID','CRASH_TIME','PERSON_TYPE','PERSON_INJURY','PERSON_AGE','COMPLAINT','CONTRIBUTING_FACTOR_1','CONTRIBUTING_FACTOR_2','PERSON_SEX']


#Filtering from people df all entries that are not victims
bol_mask = people['PED_ROLE'].ne('Registrant') & people['PED_ROLE'].ne('Notified Person') & people['PED_ROLE'].ne('Witness') & people['PED_ROLE'].ne('Policy Holder')
people_subset = people[bol_mask] 

#Since only the known boroughs will be investigated, we need to get the unique colision ID's from acc_with_borough subset
#crash_and_people = pd.merge(acc_with_borough,people_subset[wanted_from_people],how='left',left_index= True, right_index= True)
#victims_df traz a quantidade de acidentes com vítimas onde a vítima é conhecida
#victims_df = crash_and_people[crash_and_people['PERSON_TYPE'].notna()& (crash_and_people['NUMBER OF PERSONS INJURED'].gt(0))]
